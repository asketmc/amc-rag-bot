# ğŸ§  Asketmc RAG Discord Bot â€” Local LLM + Hybrid Retrieval

![Python](https://img.shields.io/badge/python-3.10%2B-blue)
![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)
![LLM](https://img.shields.io/badge/LLM-Ollama%20%7C%20Llama3-orange)
![Discord](https://img.shields.io/badge/Discord-Bot-informational)
![NLP](https://img.shields.io/badge/NLP-Stanza%20%7C%20spaCy-purple)
![Status](https://img.shields.io/badge/status-internal%20use-yellow)
![Updated](https://img.shields.io/badge/last%20update-June%202025-blueviolet)

> Lightweight hybrid-RAG Discord bot (vector + keyword fallback) for Russian-language file-based knowledge bases.  
> Built in 2 days from scratch (first-time Python) as a demonstration of LLM orchestration, retrieval QA logic, and modular design.

---

## âœ… Purpose & Highlights

This project was built **in just 2 days**, with no prior Python experience.  
It demonstrates how to build a fully functional **Retrieval-Augmented Generation (RAG)** assistant using:

- **Local LLM inference** (Ollama + Llama3)
- **Hybrid retrieval** via `llama-index` + keyword fallback
- **Full rerank pipeline** with `CrossEncoder`
- **Russian-language lemmatization** (Stanza, spaCy)
- **Secure, async-safe Discord bot** architecture

Originally developed for an RPG Minecraft server, this RAG bot is also designed to support internal tech documentation â€” e.g., for DevOps/QA teams.  
It supports structured configs, multilingual corpora, and real-world adaptation (e.g., Salesforce internal docs at Fleetcor).

---

## ğŸ” Key Features

- **Hybrid Retrieval**: vector search (BAAI/bge) + keyword fallback with score filtering
- **Async-safe rerank**: `BAAI/bge-reranker-v2-m3` with thread pool + CPU/GPU switch
- **Lemmatized KB**: Russian+English, SHA256 cache, sentence chunking, per-file indices
- **Discord Bot**: role-based access, command parsing, cooldowns, message sanitization
- **Fault Tolerance**: automatic fallback to local LLM on OpenRouter errors
- **Full Logging**: rotating logs per module (`chat`, `embedding`, `errors`, etc.)
- **Configurable**: `.env` + `config.py` + isolated prompt files (`system_prompt.txt`, etc.)
- **Flexible Backend**: tested with multiple LLMs, easily switchable via config

---

## ğŸ› ï¸ Stack

| Layer         | Tool / Library                        |
|---------------|---------------------------------------|
| LLM           | `ollama` + `llama3` (local inference) |
| Retrieval     | `llama-index` + VectorStoreIndex      |
| Embeddings    | `BAAI/bge-m3`                         |
| Rerank        | `CrossEncoder` (`bge-reranker-v2-m3`) |
| Lemmatization | `stanza`, `spaCy`, `langdetect`       |
| Bot API       | `discord.py`, `aiohttp`               |
| Infra         | `asyncio`, `rotating logs`, `.env` isolation |

---

### ğŸ”„ Model Routing

By default, the bot uses **OpenRouter (DeepSeek-v3)** for high-quality completions.  
If OpenRouter is unavailable (e.g., quota exceeded or downtime), it falls back to **local inference (Llama3-8B)** via Ollama on RTX 1060 6GB.

#### âœ… Tested / Supported Models:
- DeepSeek-v3 (OpenRouter)
- DeepSeek-v1 8B
- Phi-3 Mini
- Llama3-8B (local)

**Final choices**:
- **DeepSeek-v3** â€” for high-quality reasoning
- **Llama3-8B** â€” for reliable results under resource constraints

---

## ğŸ“ File Layout

```
/LLM
â”œâ”€â”€ bot/                        # Main bot application (entry point, core logic)
â”‚   â”œâ”€â”€ main.py                 # Entry point: init, load index, launch Discord bot
â”‚   â”œâ”€â”€ config.py               # Global settings and constants
â”‚   â”œâ”€â”€ rerank.py               # CrossEncoder reranking logic
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies (active version)
â”‚   â”œâ”€â”€ requirements-backup.txt # Backup dependency list (pip freeze)
â”‚   â”œâ”€â”€ rag_cache/              # Vector index + lemma cache
â”‚   â””â”€â”€ __pycache__/            # Python bytecode cache
â”‚
â”œâ”€â”€ parsed/                    # Input text/markdown documents for knowledge base
â”œâ”€â”€ logs/                      # Rotating log files (runtime/debug output)
â”œâ”€â”€ parsers/                   # Optional data parsers / preprocessors (WIP)
â”‚
â”œâ”€â”€ system_prompt_reason.txt   # System prompt for reasoning / verbose QA mode
â”œâ”€â”€ system_prompt_strict.txt   # System prompt for strict factual QA
â”œâ”€â”€ rephrase.txt               # Prompt for question rewriting (optional)
â”‚
â”œâ”€â”€ .env                       # Environment variables (API keys etc.)
â”œâ”€â”€ .env-example               # Example env file for configuration
â”œâ”€â”€ .gitignore                 # Git exclusion rules
â””â”€â”€ README.md                  # Project documentation

```

---

## ğŸ§ª QA & Observability

- Full trace logs (per block, per retrieval step)
- Debug mode shows similarity scores, cache hits, context chunks
- Input sanitization: char filter, length guard, unique word threshold
- Commands protected by cooldowns, admin whitelist, regex filters
- Clean fallback between vector / keyword / local LLM

---

## ğŸ’¬ Example Log â€” Step-by-Step Breakdown

```text
ğŸ“¥ [Retrieval Stage]
[INFO ] RAG: Received query â†’ "Does this character bio contain lore violations?"
[INFO ] Lemma: Loaded index with 45 documents, 0 new files detected
[INFO ] Vector Search: Top 24 chunks matched (BAAI/bge-m3)

ğŸ§  [Rerank Stage]
[DEBUG] Rerank: Using model BAAI/bge-reranker-v2-m3 (CPU), query_len=456, input_k=18
[DEBUG] Filtering 18 candidate pairs (sample: "...Night Spirit. Her followers...")
[INFO ] Rerank completed in 37.45 sec
[DEBUG] Top scores: [0.728, 0.013, 0.0027, ...]

ğŸŒ [LLM Query Stage]
[INFO ] Fallback system: OpenRouter access allowed
[INFO ] OpenRouter call started (model: DeepSeek-v3)
[INFO ] OpenRouter HTTP 200 OK
[INFO ] OpenRouter response successfully received
```

---

## ğŸ“¢ Discord Commands

- `!strict <Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ>` â€” standard RAG reply with rerank  
- `!local <Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ>` â€” local LLM-only answer  
- `!think <Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ>` â€” alternate prompt mode  
- `!reload_index` â€” admin-only index rebuild  
- `!status` â€” debug/info panel  
- `!stop` â€” admin-only shutdown

---

## ğŸ› ï¸ Setup

1. **Clone the repo**  
    ```sh
    git clone https://github.com/youruser/amc-rag-bot.git
    cd amc-rag-bot
    ```

2. **Install dependencies**  
    ```sh
    pip install -r requirements.txt
    python -m spacy download en_core_web_sm
    python -m stanza.download ru
    ```

3. **Add API keys**  
    ```ini
    DISCORD_TOKEN=...
    OPENROUTER_API_KEY=...
    ```

4. **Place your knowledge base** into `/parsed/`

5. **Run the bot**  
    ```sh
    python main.py
    ```

---

## âš¡ Why This Matters

This project shows how an **LLM QA Engineer** or **RAG Architect** can:

- Build a working hybrid retrieval system
- Handle edge cases, multilingual input, and fallback routing
- Deploy a real-time QA bot using only Python and open APIs
- Deliver results under real constraints (2 days, no prior Python background)

Use this repo as a reference, PoC baseline, or technical interview sample.

---

## ğŸ›¡ License
MIT

ğŸ“¬ Contact: asketmc.team+ragbot@gmail.com
